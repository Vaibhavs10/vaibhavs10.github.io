[
  {
    "objectID": "now.html",
    "href": "now.html",
    "title": "Now",
    "section": "",
    "text": "What am I focused on at this point in life?\nUpdated 13th April 2025, from Paris, FR.\n\nüìö Research & Work\n\nLife has been in hyper-drive since the last time I wrote one of these. My role at work (Hugging Face) has changed quite drastically, lot more high stakes and hell of a lot more fun. Be it collaborating with some of the top labs, researching Audio LMs, Text-to-Speech or about helping the team excute new product ideas/ directions - it has been a hoot and a half.\nIt‚Äôs also good realisation to take it easy every now and then, all gas and no pitstop is not as fun. That said, this job has pretty much made me an adrenaline junkie with so much cool stuff happening all around.\nI‚Äôm also quite lucky to have people who have my best interests around me at work too, looking at you Julien, Lys, and Pedro - priviliged to be working with these lads!\nFun tid-bit: I‚Äôve understood that I‚Äôm good for nothing in the mornings and usually my brain kicks-in around mid-day.\nLesson from last couple months - you/ your team is as good as the incentives they are provided with, change the incentives, you can change the outcomes!\n\n\nüôå Volunteering\n\nStill haven‚Äôt found the calm or the energy to get back to volunteering. More than the volunteering I do miss the friends I made along the way. I‚Äôm hoping I get a chance to meet some of these lovelies soon.\nOn the brightside, I‚Äôm moving on nicely from last years dramatic EuroPython :)\n\n\nüß† Mental health\n\nBasically, endurance sports is all you need to get the good old man up in the head to stop playing tricks with you! Been running and biking off-late and it has been keeping me sane whilst everything around me is in hyperdrive.\nThis also means I have a huge dependence on running/ biking to take the edge off - spend Feb/ March battling with a crazy case of spint splints & plantar fasciitis and the mind did take a good toll.\nStill, overall it‚Äôs been mostly positive and I‚Äôm happy thet there‚Äôs a consistent way to channelise energy now!\n\n\nüè° ‚ÄúHome‚Äù\n\nOof, loads of thinking about this one, whilst I really love where I live currently - it comes with it‚Äôs own side effects. Mostly comes down to how deeply integrated one is in their surroundings. I love the nature around but it still doesn‚Äôt feel like home, with friends scattered around different countries, I feel a little lost sometimes. It‚Äôs a chicken and egg problem, you need to put in the time to feel at home but at the same time because you don‚Äôt feel at home you put in time in mindless pursuits.\nHoping to figure out long term plans soon and put them in motion ^^‚Äô\n\n\n‚úàÔ∏è Travel & Public speaking\n\nAfter two years of hiatus from public speaking, I‚Äôm getting back into action this year with a few guest lectures/ workshops and talks lined up already. Going to be fun rest of the year and I‚Äôm here for it!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "VB‚Äôs thoughts, comments and musings",
    "section": "",
    "text": "30\n\n\n\n\n\n\nlife\n\n\nreflections\n\n\n\nEleven beliefs that shaped a decade ‚Äî from competition to curiosity, health to humility.\n\n\n\n\n\nNov 10, 2025\n\n\nVB\n\n\n\n\n\n\n\n\n\n\n\n\nEverything you need to know about PewDiePie‚Äôs AI setup\n\n\n\n\n\n\nllm\n\n\nllama.cpp\n\n\nmlx\n\n\nvllm\n\n\ngpt-oss\n\n\nqwen\n\n\nlocal-ai\n\n\nprivacy\n\n\nmac\n\n\nmetal\n\n\n\nRecreate PewDiePie‚Äôs $20K AI setup on your laptop. Local-first, privacy-aware, and surprisingly achievable.\n\n\n\n\n\nNov 5, 2025\n\n\nVB\n\n\n\n\n\n\n\n\n\n\n\n\nMermaid Scratchpad\n\n\n\n\n\n\ntools\n\n\n\nPaste Mermaid syntax, render instantly, copy the SVG.\n\n\n\n\n\nOct 11, 2025\n\n\nVB\n\n\n\n\n\n\n\n\n\n\n\n\nBrag Sheet\n\n\n\n\n\n\nBrag Sheet\n\n\nWork\n\n\nResearch\n\n\nCareer\n\n\n\nA chronological showcase of work I‚Äôm proud of‚Äîfrom Hugging Face releases to research breakthroughs.\n\n\n\n\n\nAug 30, 2025\n\n\nVB\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Smart Agents with MCP and OpenAI gpt-oss\n\n\n\n\n\n\nmcp\n\n\nopenai\n\n\ngpt-oss\n\n\ntiny-agents\n\n\nplaywright\n\n\nhuggingface\n\n\nhuggingface-mcp\n\n\nbrowser-automation\n\n\n\nBuild browser-automating AI agents with MCP and gpt-oss. From web scraping to Hugging Face Spaces integration.\n\n\n\n\n\nAug 13, 2025\n\n\nVB\n\n\n\n\n\n\n\n\n\n\n\n\nExport and play with your Strava data on Hugging Face\n\n\n\n\n\n\nanalytics\n\n\npython\n\n\nrunning\n\n\nstrava\n\n\nmarathon\n\n\nhugging face\n\n\nData Studio\n\n\n\nExport your Strava runs, analyze trends with SQL, and discover insights about your running journey.\n\n\n\n\n\nJul 6, 2025\n\n\nVB\n\n\n\n\n\n\n\n\n\n\n\n\nRun Phi-4 with ollama and Hugging Face\n\n\n\n\n\n\nllm\n\n\nollama\n\n\nmac\n\n\nmetal\n\n\ncuda\n\n\nphi4\n\n\nphi\n\n\n\nGet Microsoft‚Äôs most accurate Phi-4 running locally on your Mac in minutes. No GPU required.\n\n\n\n\n\nJan 16, 2025\n\n\nVB\n\n\n\n\n\n\n\n\n\n\n\n\nAll things running üèÉ‚Äç‚ôÇÔ∏è\n\n\n\n\n\n\nthings\n\n\nrunning\n\n\nnutrition\n\n\nrecovery\n\n\nmarathon\n\n\n\nTraining plans, recovery tips, nutrition hacks, and injury management for marathon prep‚Äîall in one place.\n\n\n\n\n\nDec 27, 2024\n\n\nVB\n\n\n\n\n\n\n\n\n\n\n\n\nRun Phi-3.5 with llama.cpp\n\n\n\n\n\n\nllm\n\n\nllama.cpp\n\n\nmac\n\n\nmetal\n\n\ncuda\n\n\nphi3.5\n\n\nphi\n\n\n\nRun Microsoft‚Äôs Phi-3.5 locally on any device. Small models, big performance, zero API calls.\n\n\n\n\n\nAug 22, 2024\n\n\nVB\n\n\n\n\n\n\n\n\n\n\n\n\nList of random things I keep forgetting\n\n\n\n\n\n\nthings\n\n\n\nQuick reference for commands and snippets I always need but can never remember‚Äînow saved forever.\n\n\n\n\n\nJul 29, 2024\n\n\nVB\n\n\n\n\n\n\n\n\n\n\n\n\nLive async, live happy\n\n\n\n\n\n\nasync\n\n\n\nWhy I skip meetings, ignore calls, and prefer written communication. Async is the way.\n\n\n\n\n\nJul 29, 2024\n\n\nVB\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is home?\n\n\n\n\n\n\nhome\n\n\n\nA personal reflection on belonging, vulnerability, and the places and people that make us feel at home.\n\n\n\n\n\nDec 26, 2023\n\n\nVB\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/live-async/index.html",
    "href": "posts/live-async/index.html",
    "title": "Live async, live happy",
    "section": "",
    "text": "Why I don‚Äôt take calls and take ages to respond?\nI often get my butt kicked for not responding to acquaintances and colleagues on time. I also get the stink eye for refusing to do meetings, phone/ zoom calls, and everything that comes with them.\nTo be honest, a one-hour physical meeting can be a 30-minute virtual meeting, a 30-minute virtual meeting can be a 10-minute phone call, and a 10-minute phone call can be a 2-minute email.\nI prefer written communication. It‚Äôs documented, clear, and doesn‚Äôt require me to break my focus.\nBut why avoid meetings in the first place? Isn‚Äôt face-to-face interaction better?\nI have the attention span of a peanut. It often takes me ages to find inspiration. Once I find that inspiration, I like capitalising on it as much as possible. Given how fickle my mind is I tend to aggressively preserve my ‚Äúinspired time‚Äù.\nWhile face-to-face interactions ‚Äúmay‚Äù be better. They do result in an absurd amount of cognitive load.\nSo why make life difficult for both you and me? Async rules!\nWhat about virtual meetings? They are a good proxy for in-person meetings and don‚Äôt often require as much time.\nTrue-ish, but if I know I have a meeting on a particular day, I‚Äôd constantly be stressing out about it. Irrespective of the contents of the meeting, I‚Äôd be consumed by it and have a sub-optimal experience.\nHow do I reach you?\nEmail me at Vaibhavs10[at]gmail[dot]com or drop a line on twitter @reach_vb ü§ó\nLive async, live happy!"
  },
  {
    "objectID": "posts/things-i-keep-forgetting/index.html",
    "href": "posts/things-i-keep-forgetting/index.html",
    "title": "List of random things I keep forgetting",
    "section": "",
    "text": "I often end up looking for certain commands/ code snippets so here is a list.\n\nChange the server language to English: export LC_ALL=C\nLook at the username for of who is running a process on a multi-user server: ps -u -p &lt;PROCESS_ID&gt;\nRe-authenticate with github after your PAT has expired: gh auth login -h [github.com](http://github.com/)\nPip install without clogging the cache dir, add a --no-cache-dir flag to your pip install commands.\nCreate a virtual environment with python -m venv &lt;path_to_the_environment&gt; command.\nRun a python script with nohup and pipe the output to a file. nohup python my_script_is_the_best.py &&gt; my_scripts_output.out 2&gt;&1 &\nBatch convert wav files to mp3 files with ffmpeg - for f in *.{wav,WAV}; do ffmpeg -i \"$f\" -c:a libmp3lame -q:a 2 \"${f%.*}.mp3\"; done\nAppend a prefix to specific files in a directory - for f in {0,1,2,3,4,5}.mp3; do mv \"$f\" \"GAN_$f\"; done"
  },
  {
    "objectID": "posts/30/index.html",
    "href": "posts/30/index.html",
    "title": "30",
    "section": "",
    "text": "Some beliefs that shaped the last decade:\n\nyou‚Äôre capable of more than you can imagine; every now and then, set an immeasurable goal\nyour only competition is you‚Äîeveryone else is a distraction; you‚Äôve only got a lifetime to get better\nstrive to deliver value in good faith; when in doubt, walk away\ndon‚Äôt treat money as a taboo. Money attracts money‚Äîit is never obvious in the moment, but in hindsight it is\nyour choice in partner is the most important; spend time on it, make sure there‚Äôs alignment‚Äîthis is not something you can ‚Äúwing‚Äù\npeople are not inherently bad, but some can still cause more harm than good‚Äîmute/block repeat offenders\nkeep the curiosity alive; it‚Äôs the only true antidote to boredom\nhigher highs will certainly lead to lower lows; you never recognise either before they hit you\nbe receptive to feedback, especially critical; be wary of ‚Äúgood‚Äù feedback‚Äîit‚Äôs sneaky\nhealth is wealth; this is a fact‚Äîinternalise it, practice it, make it a big part of your life\nwhen in doubt, simplify things; repeat until the next step is clear\n\nAbove all else‚Äîcreate more value than you consume, and don‚Äôt be a fucking dick!"
  },
  {
    "objectID": "posts/what-is-home/index.html",
    "href": "posts/what-is-home/index.html",
    "title": "What is home?",
    "section": "",
    "text": "Written on 26th December 2023, from Gurgaon, India.\nI‚Äôm currently in India, and as someone who doesn‚Äôt actively live in the country anymore, it is always a bittersweet experience whenever I travel between countries.\nTravelling to India üõ¨ - Is a joyride; it evokes the feeling of belongingness paired with the anticipation of meeting my loved ones, eating some good food (not butter chicken), and most of all, being in a place of comfort. It‚Äôs a place where I‚Äôve messed up umpteen numbers of times and gotten back up.\nTravelling out of India üõ´¬†- Is an emotional rollercoaster; not knowing when I will see my loved ones again evokes a feeling of emptiness, and the short life doesn‚Äôt make the departure easy. At the same time, the knowledge that I‚Äôll return to another country that I call home, living the life I chose, makes me feel elated.\nAll of this begs to answer the question: what is home? ‚ò∫Ô∏è\nI went to the mountains recently, which involved a lot of sitting in the car and a lot of time to ponder this topic.\nHere are some unkempt thoughts:\n\nIt is a place where you can be yourself. A place where you can express yourself without the fear of being judged.\nA place where you can be vulnerable.\nIt is a person who tells you you‚Äôve messed up and helps you while you fix it.\nWhere you feel motivated to get better (financially/ physically/ mentally) for your home.\nIt is a safe place where you can exist.\n\nIn the recent past (half a decade), I‚Äôve been quite lucky and privileged to build a few:\n\nMy best friend turned partner, Sofdog - My life wouldn‚Äôt be as worthwhile and fruitful as it is now if it weren‚Äôt for her watching out for me. Having tough conversations where we can be vulnerable and express ourselves without feeling judged, all whilst sharing the passion of exploring the world one city at a time.\nMum, Dad & my sister - Even though we‚Äôre often a couple of thousand kilometres apart, I‚Äôve never felt lonely all because these three have ensured I‚Äôve always been at ease. They‚Äôve always been there through thick and thin. I am lucky to have grown with such inclusive and progressive thoughts.\nFriends - Raquel, Patrick, Emils, Sven, and Zeina - Have played an important role in my life. They‚Äôve helped me max out life whilst being around me when I was hurting or guiding me when I was clueless.\nStuttgart - I spent my best life so far in Stuttgart. I got the time and space to dive deep into my research, all whilst making connections I‚Äôll cherish until the end. However, I‚Äôve moved away from Stuttgart recently. It will continue to be a place that I‚Äôll think fondly of.\n\nSo, in the end, what is home? üòä\nAs this year (2023) dawns to an end, I‚Äôm quite bullish on the next few. I can‚Äôt wait to build new homes.\nI‚Äôd love to know what makes you feel at home. Send me a DM on Twitter or drop me a ping at reachvaibhavs10[at]gmail[dot]com\nCheers! üçª VB\nAppendix - FAQ\n\nYou‚Äôve changed in recent times. That‚Äôs correct; I am a bit snobbier now (always a delhite); I take things way more lightly now. I‚Äôm more direct (don‚Äôt mince my words). I‚Äôm sorry if my words hurt you.\nDo I still have a tolerance for Spice? No.¬†It causes me intense pain in the belly. And no, I do not want to build it back up.\nDo I intend to come back to India to live long-term? Not in the immediate future (&lt; 2 years), and I do not plan life more than two years in advance. I‚Äôll continue to visit, albeit a bit more frequently now.\nWhat do you not like about India? First, I love Delhi, and it‚Äôll forever be my home. That said, I don‚Äôt like the political climate in India. I despise the pollution. The traffic is nightmare fuel. And no, this is not up for discussion. I find conversations like those to be not worth your or my time.\nPeople in tech get paid much more in India than in Europe. That‚Äôs great, and I‚Äôm quite happy for the tech scene in India.\nDo I make good money? I make enough to have food on the table and some occasional trips here and there. That‚Äôs more than sufficient for me. But, deffo something I will improve in ‚Äô24."
  },
  {
    "objectID": "posts/run-phi-on-device-llama-cpp/index.html",
    "href": "posts/run-phi-on-device-llama-cpp/index.html",
    "title": "Run Phi-3.5 with llama.cpp",
    "section": "",
    "text": "I‚Äôm a strong believer that as we continue to traverse the AI/ ASI/ AGI landscape, the net compute required for accessing it would decrease i.e.¬†we‚Äôll get smaller but stronger models.\nAs these models continue to become smaller and stronger, it‚Äôs more likely that you can run inference directly on your device and replace your on-line/ API based LLM usage (like ChatGPT, Gemini, Claude, etc). Hence, I continue to keep an eye out for smaller models.\nThe big question here, is what use-cases do you actually use these on-device models for?\nI use them for a few select task, like, rewriting, summarizing, quick syntax lookups, or just quick vibe checks on a topic (asking the model to critique what I‚Äôve written) and creative explorations.\nThis brings us to Phi 3.5. The other day, Microsoft released an update to their Phi series of small scale LLMs: Phi 3.5.\nThe highlight: Phi 3.5 mini, MoE and vision with 128K context, multilingual & MIT license! üî•\nThe MoE beats Gemini flash and the Vision model is competitive with GPT4o.\nSome quick notes on the models and they atleast look good on benchmarks:\nMini with 3.8B parameters\nMoE with 16x3.8B (6.6B active - 2 experts)\nPh3.5 Vision with 4.2B params\nThe star of the show for me atleast is the Phi-3.5 Mini, it looks like decently sized model which would make it easy to fine-tune for specific tasks.\nAlright, enough of about the models, let‚Äôs try to see if we can run this model on your Mac/Windows/Linux device.\nThere are multiple ways to run a model on-device - you can use, transformers, llama.cpp, MLC, ONNXRuntime and bunch others. One of the easiest ways however is to use llama.cpp, once setup - it just works across pretty much all well-known model architectures."
  },
  {
    "objectID": "posts/run-phi-on-device-llama-cpp/index.html#step-1-setup-llama.cpp",
    "href": "posts/run-phi-on-device-llama-cpp/index.html#step-1-setup-llama.cpp",
    "title": "Run Phi-3.5 with llama.cpp",
    "section": "Step 1: Setup llama.cpp",
    "text": "Step 1: Setup llama.cpp\nOn a mac, you can install llama.cpp using homebrew:\nbrew install llama.cpp\nOn a Windows/ Linux device, you can follow the instructions on the llama.cpp Docs)\nFor example, a typical build command for CUDA enabled device looks like this:\ngit clone https://github.com/ggerganov/llama.cpp && \\\ncd llama.cpp && \\\nmake GGML_CUDA=1 LLAMA_CURL=1\nThere are plenty other permutations and combinations so feel free to look at those on the linked docs above."
  },
  {
    "objectID": "posts/run-phi-on-device-llama-cpp/index.html#step-2-run-inference-with-phi3.5",
    "href": "posts/run-phi-on-device-llama-cpp/index.html#step-2-run-inference-with-phi3.5",
    "title": "Run Phi-3.5 with llama.cpp",
    "section": "Step 2: Run inference with Phi3.5",
    "text": "Step 2: Run inference with Phi3.5\nTo be able to run a LLM on-device all you need is to find the model on the hub and simply point the llama.cpp‚Äôs built binaries to it. There are over 30,000 quantized llama.cpp models on the Hub. For our use-case we‚Äôll use the lmstudio-community/Phi-3.5-mini-instruct-GGUF repo by the good folks at lm-studio.\nllama-cli --hf-repo lmstudio-community/Phi-3.5-mini-instruct-GGUF \\\n--hf-file Phi-3.5-mini-instruct-Q6_K.gguf \\\n-p \"The meaning to life and the universe is\" -c 8192"
  },
  {
    "objectID": "posts/run-phi-on-device-llama-cpp/index.html#step-3-use-it-for-your-own-tasks",
    "href": "posts/run-phi-on-device-llama-cpp/index.html#step-3-use-it-for-your-own-tasks",
    "title": "Run Phi-3.5 with llama.cpp",
    "section": "Step 3: Use it for your own tasks",
    "text": "Step 3: Use it for your own tasks\nThat‚Äôs the fun bit, once the model is loaded, you can do whatever you want, at the touch of your terminal.\nGo on and try out some of your own prompts, and see how it works. There‚Äôs myriads of options you can use whilst playing with the model, check out the llama.cpp docs for more details.\nHere‚Äôs some of the options I use quite a bit:\nllama-cli --hf-repo bartowski/Phi-3.5-mini-instruct-GGUF \\\n--hf-file Phi-3.5-mini-instruct-Q8_0.gguf \\\n-p \"Hey my name is Sof, what are you upto?\" -c 8192 -cnv --color\n-c is the context window size, -cnv is to use the conversation/ chat mode and --color is to colorize the output.\nBonus: Now go try other GGUF models on the Hub and compare their performance with Phi 3.5.\nand.. that‚Äôs it!"
  },
  {
    "objectID": "posts/all-things-running/index.html",
    "href": "posts/all-things-running/index.html",
    "title": "All things running üèÉ‚Äç‚ôÇÔ∏è",
    "section": "",
    "text": "I love running, and have recently embarked on the journey to race a marathon in June 2025. In the past, I‚Äôve reached half-marathon distances multiple times, but have never went higher than that.\nThis page is a collection of resources and tips I‚Äôve found online, that I either have found useful or would like to try out in the future."
  },
  {
    "objectID": "posts/all-things-running/index.html#training",
    "href": "posts/all-things-running/index.html#training",
    "title": "All things running üèÉ‚Äç‚ôÇÔ∏è",
    "section": "Training",
    "text": "Training\n\nIt‚Äôs recommended to follow a training plan, something that mixes speed work with longer runs. I‚Äôm using Runna to generate and follow a plan (it‚Äôs a bit expensive ~20 EUR/month).\nGet a watch that at the very least tracks splits/ time elapsed. I recently upgraded to a Apple Watch Series 10 (~460 EUR) and it does it‚Äôs job. DC Rainmaker has a good review of all.\nComfy clothes that make you feel comfortable whilst running - decathalon has a very good selection (specially for winters), you don‚Äôt need to spend a lot on this - but get this right, specially for winters - good read.\nShoes - again this is quite vibe based, you cannot go wrong either way, this requires a bit of experimentation ofc - for ref I‚Äôve been using 80 EUR Puma running shoes for past 1.5 years, and it‚Äôs pretty good - another worthwhile read.\nOptional but recommended: create a Strava account, it‚Äôs a great way to keep yourself motivated and track your progress. Here‚Äôs mine.\nA fun way to keep yourself motivated is to run races leading up to the marathon, these could be 10Ks, half marathons or just fun runs (keeps the goal achievable)."
  },
  {
    "objectID": "posts/all-things-running/index.html#recovery",
    "href": "posts/all-things-running/index.html#recovery",
    "title": "All things running üèÉ‚Äç‚ôÇÔ∏è",
    "section": "Recovery",
    "text": "Recovery\n\nIt‚Äôs good to walk a KM or two after runs, it helps bring the heart rate down and helps ‚Äúsettle‚Äù the legs - good discussion.\nWhilst the jury is out on the need of stretching, I‚Äôve anecdotaly found static stretches to help feel quite a bit flexible the rest of the day - here‚Äôs what I follow.\nIt‚Äôs a good idea to introduce one or two rest days in the week - a good reddit thread.\nMagnesium helps with sore legs (I usually take 500mg after a long run) - good read.\nTake a nice long shower, you deserve it, and you probably stink a lot too ;)"
  },
  {
    "objectID": "posts/all-things-running/index.html#nutrition",
    "href": "posts/all-things-running/index.html#nutrition",
    "title": "All things running üèÉ‚Äç‚ôÇÔ∏è",
    "section": "Nutrition",
    "text": "Nutrition\nFew basic things I‚Äôve learned:\n\nMake sure to drink enough water 30-45 minutes before the run, you can even take a bit of electrolytes (although not too much).\nChase up the run with a protein shake and chocolate milk, and drink loads of water in the hours after the run. It‚Äôs almost critical to eat something within 30 minutes of your run.\nAvoid over caffeinating before the run (I‚Äôve experimented a bit and I do well as long as I‚Äôve had one coffee, more than that makes my heart race a bit too much).\nDon‚Äôt eat a big breakfast or anything that takes a long time to digests one hour before the run (avoid dairy, fiber and fruits).\nHaving a carb rich meal before the run helps feeling overall more energetic throughout the run (I‚Äôve just started experimenting with this).\n\nUp to 15 KMs\n\nCarry on as you would for a normal run, but make sure to hydrate well after the run.\nDon‚Äôt run till you hit a wall, everyone has different energy consumption, learn your limits, running on fumes almost always causes injury/ cramps.\nIt‚Äôs worth getting atleast two seperate running shoes, which you alternate between runs - this helps with overuse injuries.\nAs long as it‚Äôs more than an hour, taking a gel is more than recommended.\n\nUp to 30KMs\n\nTake up 1-2 isotonic + energy gels, the goal is to get 60-80g catbs per hour of your run. (As of the date of the post, I haven‚Äôt tried this yet)"
  },
  {
    "objectID": "posts/all-things-running/index.html#shin-splints",
    "href": "posts/all-things-running/index.html#shin-splints",
    "title": "All things running üèÉ‚Äç‚ôÇÔ∏è",
    "section": "Shin splints",
    "text": "Shin splints\nLast week (week of 19 Jan), I ramped up my running to 45KMs a week AND this was after an intense week of skiing. It was a bit stupid and straight up delusional for me to subject myself to that - but, well, we live and learn.\nHere‚Äôs some stuff I‚Äôve realised and learnt in the past week:\n\nYou just gotta wait it out, and do as little as possible for a few days (even avoid walking as much).\nIce that bastard, even multiple times a day - I‚Äôd just use a frozen pack of peas for 15-20 minutes at a time.\nDo really guided and slow foam rolling, it takes some getting used to, but it‚Äôs a good way to release some knots and tension in the shins/ calves.\nThe thing that worked the best is to massage around and directly at the shin splint area using both your thumbs, specially in the early days it can hurt a bit, but it gets better progressively.\nThis is something reddit has recommended quite a bit, here\n\nLesson learnt: don‚Äôt go too hard too fast, and don‚Äôt be too stubborn to take it easy."
  },
  {
    "objectID": "posts/all-things-running/index.html#plantar-fasciitis",
    "href": "posts/all-things-running/index.html#plantar-fasciitis",
    "title": "All things running üèÉ‚Äç‚ôÇÔ∏è",
    "section": "Plantar fasciitis",
    "text": "Plantar fasciitis\nAfter recovering a bit from the shin splints I ended up getting a pretty bad case of PF.\nHere‚Äôs some stuff I‚Äôve learnt about it:\n\nBuy shoes after measuring your feet and the size of your upper box specifically, most cases stem from super tight shoes.\nDo not walk barefoot, for couple months - buy a pair of birkenstock and stick with them for couple months.\nReally load up your legs specifically your calves - squats, lunges, calf raises and one leg RDLs are your best friend.\nDon‚Äôt stop running, keep accumulating slow kms (even if it‚Äôs walking speed).\nIt‚Äôs going to be awhile before you feel 100% again - but it‚Äôs okay, you‚Äôll be fine!"
  },
  {
    "objectID": "posts/all-things-running/index.html#zone-2-running",
    "href": "posts/all-things-running/index.html#zone-2-running",
    "title": "All things running üèÉ‚Äç‚ôÇÔ∏è",
    "section": "Zone 2 Running",
    "text": "Zone 2 Running\nFor the past 4-5 weeks (from 25th May) I‚Äôve been experimenting doing a lot of volume at Easy pace, and I mean really easy pace. The real motivation was to reduce the number of times I got injured whilst trying to build volume.\nThat said, it‚Äôs brutal and humbles you quite quickly. As someone who suffered from Asthma growing up and basically kept an arms length away from sports it‚Äôs been surreal trying to run slowly.\nHere‚Äôs some stuff I‚Äôve learnt about it:\n\nIt‚Äôs going to be tough. Jogging is your best friend.\nGo slow but try to go long (even if it means that you walk for prolonged periods of time).\nMeasure your heart rate, try to stay within the zone, ofc if you go uphill hte you might dip into Z3, it‚Äôs okay - don‚Äôt overthink it.\nTry to add some strides before cool down, they are a good way to keep the fun alive.\nBe prepared to be humbled, everyone and their mums will be faster than you on strava, remember, it‚Äôs all for you - fuck the rest.\nTry to focus on cadence, try to keep it above 165 (it‚Äôll help you avoid injuries).\nIt‚Äôs going to get better sooner than you think.\n\natm I‚Äôm average from 07:00-08:30/ km in Zone 2, I started at 09:30/ km 5 weeks back, you build good posture and cadence back."
  },
  {
    "objectID": "posts/mcp-with-openai-gpt-oss/index.html",
    "href": "posts/mcp-with-openai-gpt-oss/index.html",
    "title": "Building Smart Agents with MCP and OpenAI gpt-oss",
    "section": "",
    "text": "[!NOTE] This post is to be accompanied by it‚Äôs GitHub repository here\nIn this guide, we‚Äôll explore using MCP and demonstrate how to build AI agents using gpt-oss as the LLM backbone. We‚Äôll use Hugging Face‚Äôs lightweight MCP clients:\nThe key to building effective AI agents lies in their tools. MCP provides a standardized interface for tool interaction, making it simple to create powerful agents.\nLet‚Äôs dive in by creating a web-savvy agent that can browse and search the internet for you."
  },
  {
    "objectID": "posts/mcp-with-openai-gpt-oss/index.html#example-local-browser-agent",
    "href": "posts/mcp-with-openai-gpt-oss/index.html#example-local-browser-agent",
    "title": "Building Smart Agents with MCP and OpenAI gpt-oss",
    "section": "Example: Local Browser Agent",
    "text": "Example: Local Browser Agent\nLet‚Äôs build a browser agent that can browse and search the internet for you. We‚Äôve already implemented this in /browser-agent directory in case you want to skip ahead.\n\nStep 0: Log in to Hugging Face\nhuggingface-cli login\nThis will ask you for your HF API token. You can get it from here.\n\n\nStep 1: Define the Agent\nBoth the JS and Python Tiny agent clients are meant to be quite easy to play and experiment with. They expect a transparent agent.json which includes the details of which LLM should be used and what tools it should have access to.\nLet‚Äôs define our agent using OpenAI‚Äôs latest gpt-oss 120B as the LLM and connect it to this Playwright MCP server that provides browser automation capabilities to your agent. We‚Äôve added this in /browser-agent/agent.json for you to use.\n{\n    \"model\": \"openai/gpt-oss-120b\",\n    \"provider\": \"fireworks-ai\",\n    \"servers\": [\n        {\n            \"type\": \"stdio\",\n            \"command\": \"npx\",\n            \"args\": [\"@playwright/mcp@latest\"]\n        }\n    ]\n}\nOptionally, we can define a System Prompt that helps steer the LLM. This is defined in /browser-agent/PROMPT.md for you to use.\n\nYou are an agent - please keep going until the user‚Äôs query is completely resolved, before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved, or if you need more info from the user to solve the problem.\nIf you are not sure about anything pertaining to the user‚Äôs request, use your tools to read files and gather the relevant information: do NOT guess or make up an answer.\nYou MUST plan extensively before each function call, and reflect extensively on the outcomes of the previous function calls. DO NOT do this entire process by making function calls only, as this can impair your ability to solve the problem and think insightfully.\nHelp the User with their task.\n\n\nThat's it, let's take it out for a spin.\n\n### Step 2: Run the agent\n\nTo run the agent in Python, we simply install the `tiny-agents` package, which is part of the `huggingface_hub` library.\n\n```bash\npip install -U \"huggingface_hub[mcp]&gt;=0.32.0\"\nFollowed by running our agent:\ntiny-agents run ./browser-agent\n\n\n\nCLI Init Browser\n\n\nWhen the agent starts, you can chat with it to ask him to solve tasks. For example, try to ask it to find the top 10 Hugging Face models, and see if it‚Äôs able to connect to the website using the Playwright MCP tool we configured!\n\n[!NOTE] You can do exactly the same thing with our JavaScript client as well.\nnpx @huggingface/tiny-agents run ./browser-agent\n\n\n\n\nBrowser Agent Demo\n\n\nVoila, you now have a capable browser agent with you!"
  },
  {
    "objectID": "posts/mcp-with-openai-gpt-oss/index.html#example-accessing-hugging-face-mcp-servers",
    "href": "posts/mcp-with-openai-gpt-oss/index.html#example-accessing-hugging-face-mcp-servers",
    "title": "Building Smart Agents with MCP and OpenAI gpt-oss",
    "section": "Example: Accessing Hugging Face MCP Servers",
    "text": "Example: Accessing Hugging Face MCP Servers\nLet‚Äôs take it up a notch and give more creative freedom to our AI Agent, cue, Hugging Face MCP Server. The HF MCP server allows you to not only interact with the HF Hub but also with 1000s of AI spaces on hf.co/spaces.\nLet‚Äôs get it set up!"
  },
  {
    "objectID": "posts/mcp-with-openai-gpt-oss/index.html#step-1-find-an-mcp-server",
    "href": "posts/mcp-with-openai-gpt-oss/index.html#step-1-find-an-mcp-server",
    "title": "Building Smart Agents with MCP and OpenAI gpt-oss",
    "section": "Step 1: Find an MCP Server",
    "text": "Step 1: Find an MCP Server\nHead over to hf.co/mcp and add the spaces/ demo that you want to be able to play with\n\n\n\nHugging Face MCP Settings page\n\n\nFor example, I‚Äôve added the following space\n\nevalstate/FLUX.1-Krea-dev - a popular aesthetic text to image model by Black Forest Labs\nevalstate/ltx-video-distilled - a popular image/ text to video by Lightricks\n\nNext, let‚Äôs update our agent.json:\n{\n    \"model\": \"openai/gpt-oss-120b\",\n    \"provider\": \"fireworks-ai\",\n    \"inputs\": [\n        {\n            \"type\": \"promptString\",\n            \"id\": \"hf-token\",\n            \"description\": \"Your Hugging Face Token\",\n            \"password\": true\n        }\n    ],\n    \"servers\": [\n        {\n            \"type\": \"http\",\n            \"url\": \"https://huggingface.co/mcp\",\n            \"headers\":\n            {\n                \"Authorization\": \"Bearer ${input:hf-token}\"\n            }\n        }\n    ]   \n}\n\nStep 2: Run it!\nLet‚Äôs run it with Tiny agents just like we did with the local browser agent.\ntiny-agents run ./hf-mcp-server\n\n[!NOTE] Again, you can do exactly the same thing with our JavaScript client as well.\nnpx @huggingface/tiny-agents run ./hf-mcp-server\n\n\n\n\nWatch the demo video\n\n\nThat‚Äôs it! What would you build next with it?"
  },
  {
    "objectID": "posts/run-phi-on-device-ollama/index.html",
    "href": "posts/run-phi-on-device-ollama/index.html",
    "title": "Run Phi-4 with ollama and Hugging Face",
    "section": "",
    "text": "This is going to be a short post or rather a testlog for how to run the most accurate version of Microsoft Phi-4 on your Mac."
  },
  {
    "objectID": "posts/run-phi-on-device-ollama/index.html#step-1-setup-ollama",
    "href": "posts/run-phi-on-device-ollama/index.html#step-1-setup-ollama",
    "title": "Run Phi-4 with ollama and Hugging Face",
    "section": "Step 1: Setup ollama",
    "text": "Step 1: Setup ollama\nOn a mac, you can install ollama using homebrew:\nbrew install ollama\nOn a Windows/ Linux device, you can follow the instructions on the Ollama Docs)"
  },
  {
    "objectID": "posts/run-phi-on-device-ollama/index.html#step-2-kickstart-ollama",
    "href": "posts/run-phi-on-device-ollama/index.html#step-2-kickstart-ollama",
    "title": "Run Phi-4 with ollama and Hugging Face",
    "section": "Step 2: Kickstart ollama",
    "text": "Step 2: Kickstart ollama\nollama serve"
  },
  {
    "objectID": "posts/run-phi-on-device-ollama/index.html#step-3-run-inference-with-phi-4",
    "href": "posts/run-phi-on-device-ollama/index.html#step-3-run-inference-with-phi-4",
    "title": "Run Phi-4 with ollama and Hugging Face",
    "section": "Step 3: Run inference with Phi-4",
    "text": "Step 3: Run inference with Phi-4\nAfter some research I found that the Phi-4 GGUFs from Unsloth are the most accurate. They ran bunch of evals and also converted the model to LLaMa format. You can find it here: unsloth/phi-4-GGUF.\nollama run hf.co/unsloth/phi-4-GGUF\nI‚Äôd also recommend reading the blogpost about the fixes for the Phi-4 model here."
  },
  {
    "objectID": "posts/run-phi-on-device-ollama/index.html#step-4-use-it-for-your-own-tasks",
    "href": "posts/run-phi-on-device-ollama/index.html#step-4-use-it-for-your-own-tasks",
    "title": "Run Phi-4 with ollama and Hugging Face",
    "section": "Step 4: Use it for your own tasks",
    "text": "Step 4: Use it for your own tasks\nThat‚Äôs the fun bit, once the model is loaded, you can do whatever you want, at the touch of your terminal.\nGo on and try out some of your own prompts, and see how it works.\n\n\n\nChatting with Phi 4 on my Macbook with Ollama\n\n\nBonus: Now go try other GGUF models on the Hub and compare their performance with Phi 4.\nand.. that‚Äôs it!\nOh, sorry, one last thing, you can now even run private GGUFs from the Hugging Face Hub via ollama, read here."
  },
  {
    "objectID": "posts/bragsheet/index.html",
    "href": "posts/bragsheet/index.html",
    "title": "Brag Sheet",
    "section": "",
    "text": "Brag Sheet\nEveryone should put together a brag sheet, show off their body of work and most importantly be proud of what they‚Äôve accomplished so far!\nIn chronological order, stuff I‚Äôm proud in my work life:\n\nHugging Face (2022 - Now)\nI play multiple roles at HF and switch between them when needed - Project Manager, Developer Advocacy, Machine Learning Engineer, Research Scientist all whilst being Individual Contributor.\nWorked across Monetisation, Backend, Frontend, Advocacy, Research and Open Source teams.\n\nLed the release of some of the most impactful open models (OpenAI gpt-oss, Meta Llama 3 and above, Google Gemma and more)\nScaled pivotal initiatives like Enterprise Hub, Notebooks, Inference Providers, ZeroGPU from scratch\nBuilt tooling to foster a budding on-device ecosystem with llama.cpp, MLX, ONNX, for context a single space resulted in ~21K active GGUFs\nSet standards for evaluations across Speech Recognition and Text to Speech with Open ASR Leaderboard + TTS Arena\nHelped SmolLM series of models by advising on model capabilities and integrating them with WebGPU (MLC), llama.cpp, MLX\nSpent time terminally online cultivating community feedback and dog fooding our offerings (probably my fav part of the job)\nCreated hundereds of artefacts to build an ecosystem across HF offerings via blog, github, x dot com, linkedin\nMost importantly, put together a team of DevRels and on-device engineers I believe in more than myself\n\nThis isn‚Äôt all a solo effort, thousands of slack messages, PRs and a team that I can count on. In addition to the HF leadership team and specifically Julien who continued to encourage me to take risky bets and see them through.\n\n\nResearch (2020 - 2023)\n\nWork on alternate attention methods Nystr√∂mformer and Reformer for Auto Regressive Text to Speech (Tacotron + FastSpeech 2)\nBuilt diffusion based post-training network for TTS (FastSpeech 2 + Shallow Diffusion)\n\n\n\nLife\nI have pivoted four times in my life so far and it‚Äôs been a joy each time:\n\nSpent 2015-17 worked in sales and independent consulting for startups in India\n2017-2020 served as a Data Science/ Management consultant for Deloitte - helped close &gt; 4M$ deals\n2020-2023 researched NLP & Speech in Stuttgart, Germany\n2022-Now leading Developer Experience and Community at Hugging Face\n\nEach of these have taught me skills which I use everyday."
  },
  {
    "objectID": "posts/play-with-strava-data-on-hugging-face/index.html",
    "href": "posts/play-with-strava-data-on-hugging-face/index.html",
    "title": "Export and play with your Strava data on Hugging Face",
    "section": "",
    "text": "A fun exercise to export all the data from Strava over to a dataset so that I can create fun views and charts on top.\nPre-requisites:\n\nCreate an API application: https://www.strava.com/settings/api in your strava account (set localhost as the website and other details)\nSet API keys as Environment variables STRAVA_CLIENT_ID & STRAVA_CLIENT_SECRET\n\nYou can set the env variables simply by export STRAVA_CLIENT_ID=... & export STRAVA_CLIENT_SECRET=...\nFind the codebase here: https://github.com/Vaibhavs10/strava-analyse\nSetup the env and scraper:\n\nClone the github repository via: git clone https://github.com/Vaibhavs10/strava-analyse.git\nSetup the python env with uv: uv venv --python 3.12, followed by, source .venv/bin/activate\nInstall all required packages via uv pip install requests huggingface_hub\nRun huggingface-cli login (required to upload the dataset to Hugging Face)\nRun the python script and follow the instructions: python upload-strava-to-hf.py\n\nNote: when you run the script you‚Äôll be prompted to authorise access to your strava App. Once you click Authorise it‚Äôll redirect you too a page that doesn‚Äôt exist üëÄ.\nDon‚Äôt worry about it and look at the URL it is trying to redirect too, it should look something like http://localhost:8000/?state=&code=e55c038bcf96ea6deff15c68649afc9554e6fbd6&scope=read,activity:read_all,profile:read_all The thing that matters to us is the string after code, just copy and paste it to the script and that‚Äôs it!\nIf all goes well, you should be able to go to your dataset on Hugging Face, here‚Äôs mine for example: https://huggingface.co/datasets/reach-vb/strava-stats\n\n\n\nStrava Dataset\n\n\nWith the Hugging Face dataset comes a lot of interesting possiblilities to identify trends using SQL or even pandas/ polars too. Since the dataset is small we can just run SQL queries via DataStudio.\nLet‚Äôs crunch some numbers!\nTo start with, how many cummulative kms have I run through the years?\nBest part about Datastudio is you can just ask the AI to write a SQL query for you üî•\n\n\n\nData Studio AI Query\n\n\nWITH run_stats AS (\n    SELECT\n        EXTRACT(YEAR FROM start_date::DATE) AS year,\n        SUM(distance) / 1000 AS total_distance_km,\n        SUM(moving_time) AS total_moving_time_seconds\n    FROM\n        train\n    WHERE\n        type = 'Run'\n        AND distance &gt; 0\n        AND moving_time &gt; 0\n    GROUP BY\n        year\n)\nSELECT\n    year,\n    total_distance_km,\n    -- Calculate minutes and seconds separately for proper pace format\n    FLOOR((total_moving_time_seconds/60) / total_distance_km) || ':' || \n    LPAD(FLOOR(MOD((total_moving_time_seconds/60) / total_distance_km, 1) * 60)::VARCHAR, 2, '0') AS avg_pace_per_km,\n    bar(total_distance_km, 0, MAX(total_distance_km) OVER (), 30) AS distance_chart,\n    bar((total_moving_time_seconds/60) / total_distance_km, 0, MAX((total_moving_time_seconds/60) / total_distance_km) OVER (), 30) AS pace_chart\nFROM\n    run_stats\nORDER BY\n    year;\nResult:\n\n\n\n\n\n\n\n\n\n\nyear\ntotal_distance_km\navg_pace_per_km\ndistance_chart\npace_chart\n\n\n\n\n2020\n78.129\n7:18\n‚ñà‚ñà‚ñà‚ñà‚ñå\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n\n\n2021\n414.0414\n6:27\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå\n\n\n2022\n169.5316\n5:59\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå\n\n\n2023\n73.4308\n6:48\n‚ñà‚ñà‚ñà‚ñà‚ñé\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ\n\n\n2024\n363.506\n6:31\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä\n\n\n2025\n512.7066\n7:16\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä\n\n\n\nI was at my fastest self in 2022 (though with much less kms). 2023 was the least I‚Äôve ran across the years and 2025 (so far) has been the most I‚Äôve ran (albeit with a much slower pace owing too Zone 2 training).\nNext, let‚Äôs look more deeply into the year so far and how the runs have been shaping up.\nSELECT\n    EXTRACT(MONTH FROM start_date::DATE) AS month,\n    COUNT(*) AS num_runs,\n    ROUND(SUM(distance) / 1000, 1) AS total_distance_km,\n    ROUND(AVG(average_heartrate), 1) AS avg_heartrate,\n    -- Calculate pace in min:sec per km from moving_time and distance\n    CONCAT(\n        FLOOR((SUM(moving_time) / SUM(distance) * 1000) / 60),  -- minutes\n        ':', \n        LPAD(CAST(FLOOR(MOD(SUM(moving_time) / SUM(distance) * 1000, 60)) AS VARCHAR), 2, '0')  -- seconds with leading zero\n    ) AS avg_pace_min_per_km,\n    ROUND(AVG(distance) / 1000, 2) AS avg_distance_per_run_km,\n    ROUND(AVG(average_watts), 1) AS avg_power\nFROM \n    train\nWHERE \n    type = 'Run' \n    AND distance &gt; 2000\n    AND EXTRACT(YEAR FROM start_date::DATE) = 2025\nGROUP BY \n    EXTRACT(MONTH FROM start_date::DATE)\nORDER BY \n    month;\nResult:\n\n\n\n\n\n\n\n\n\n\n\n\nmonth\nnum_runs\ntotal_distance_km\navg_heartrate\navg_pace_min_per_km\navg_distance_per_run_km\navg_power\n\n\n\n\n1\n8\n70.7\n161.6\n6:24\n8.84\n176.2\n\n\n2\n9\n52.2\n163.3\n6:27\n5.81\n184.4\n\n\n3\n8\n46.3\n153.9\n7:15\n5.78\n161.8\n\n\n4\n14\n82.7\n157.5\n6:46\n5.91\n178.7\n\n\n5\n15\n104.2\n152.7\n7:59\n6.94\n157.7\n\n\n6\n16\n119.1\n154.5\n7:51\n7.44\n162.7\n\n\n7\n4\n34.0\n152.0\n7:19\n8.50\n160.6\n\n\n\nJune was the month with the most kms so far. It‚Äôs nice to see Zone 2 progression a bit from May -&gt; July (7:59 to 7:19 min/ km). As someone with the aerobic base of a potato it‚Äôs quite nice to see some progress.\nAlright, let‚Äôs delve a bit more deeper into how my Zone 2 running transformation is going\nSELECT\n    start_date::DATE AS date,\n    ROUND(SUM(distance) / 1000, 1) AS total_distance_km,\n    ROUND(AVG(average_heartrate), 1) AS avg_heartrate,\n    -- Properly formatted pace as MM:SS with leading zero for single-digit seconds\n    CONCAT(\n        FLOOR(SUM(moving_time) / SUM(distance) * 1000 / 60)::INTEGER,  -- minutes\n        ':',\n        LPAD(\n            (ROUND(SUM(moving_time) / SUM(distance) * 1000) % 60)::INTEGER::VARCHAR,  -- seconds converted to integer first\n            2, '0'\n        )\n    ) AS avg_pace_min_per_km,\n    ROUND(AVG(average_watts), 1) AS avg_power\nFROM \n    train\nWHERE \n    type = 'Run' \n    AND distance &gt; 7500  -- Filter for runs greater than 7.5 km\n    AND EXTRACT(YEAR FROM start_date::DATE) IN (2024, 2025)  -- Only 2024 and 2025 runs\nGROUP BY \n    start_date::DATE\nORDER BY \n    date ASC  -- Order from oldest to latest\nResults:\n\n\n\n\n\n\n\n\n\n\ndate\ntotal_distance_km\navg_heartrate\navg_pace_min_per_km\navg_power\n\n\n\n\n2025-01-18\n17.1\n164.3\n6:23\n177.5\n\n\n2025-02-15\n10.0\n170.1\n6:30\n184.3\n\n\n2025-02-20\n7.5\n167.3\n6:29\n184.3\n\n\n2025-04-12\n10.0\n170.0\n6:51\n181.0\n\n\n2025-04-17\n8.1\n156.8\n6:41\n181.3\n\n\n2025-05-11\n9.2\n156.0\n8:06\n152.0\n\n\n2025-05-13\n7.5\n153.5\n8:50\n139.8\n\n\n2025-05-15\n8.0\n153.3\n8:26\n149.9\n\n\n2025-05-17\n8.0\n150.1\n8:07\n157.0\n\n\n2025-05-20\n9.1\n155.1\n8:26\n149.2\n\n\n2025-05-24\n7.8\n152.0\n7:58\n157.0\n\n\n2025-05-27\n10.1\n161.7\n7:13\n171.4\n\n\n2025-06-03\n8.5\n153.0\n7:54\n159.2\n\n\n2025-06-06\n8.0\n150.8\n8:17\n152.2\n\n\n2025-06-08\n10.5\n153.3\n8:15\n152.3\n\n\n2025-06-14\n12.1\n150.4\n7:54\n164.3\n\n\n2025-06-21\n9.0\n161.0\n7:27\n167.3\n\n\n2025-06-22\n11.0\n154.0\n7:08\n174.1\n\n\n2025-06-24\n8.0\n158.7\n7:09\n181.3\n\n\n2025-06-28\n10.0\n152.9\n8:07\n153.9\n\n\n2025-07-01\n10.1\n154.0\n7:11\n0.0\n\n\n2025-07-05\n13.2\n152.5\n7:37\n160.6\n\n\n\nPretty dope to see the heart rate go down and the pace slightly improve, hoping to double down on this more as the year continues!\nThese were just simple examples but you can really do a lot more with this, checkout segemnts, bike rides power comparisons based on elevation and more.\nP.S. You can save all of these queries in your Hugging Face Data Studio as well, so you can just refresh your dataset monthly/ weekly and analyse trends.\nNow go ahead and play with it and let me know how it goes. ü§ô"
  },
  {
    "objectID": "posts/mermaid-scratchpad/index.html",
    "href": "posts/mermaid-scratchpad/index.html",
    "title": "Mermaid Scratchpad",
    "section": "",
    "text": "Drop a Mermaid diagram definition into the editor and see the rendered output on the right. Works offline once the page loads, so it‚Äôs handy when sketching architecture or flows.\n\n  \n    Paste Mermaid definition\n    graph TD\n  A[Start] --&gt; B{Is it working?}\n  B --&gt;|Yes| C[Ship it]\n  B --&gt;|No| D[Fix it]\n    Auto-renders as you type. Tap Render to refresh manually.\n  \n  \n    \n      Waiting for diagram‚Ä¶\n      \n        Render\n        Fullscreen"
  },
  {
    "objectID": "posts/everything-you-need-to-know-about-pewdiepie-s-ai-setup/index.html",
    "href": "posts/everything-you-need-to-know-about-pewdiepie-s-ai-setup/index.html",
    "title": "Everything you need to know about PewDiePie‚Äôs AI setup",
    "section": "",
    "text": "TL;DR - Watch Felix Kjellberg‚Äôs ‚ÄúSTOP. Using AI Right now.‚Äù (31 October 2025) to see his 10‚ÄëGPU setup and custom UI: https://www.youtube.com/watch?v=qw4fDU18RcU. - You can recreate the workflow with consumer gear‚Äîstart with 7B‚Äì20B quantized models in llama.cpp or Apple‚Äôs MLX, then scale up if you add VRAM. - The real goal is autonomy: keep data off third-party APIs, wire in your own search/memory, and iterate until the tooling feels personal.\nFelix ‚ÄúPewDiePie‚Äù Kjellberg recently shared a video showing his $20K, 10‚ÄëGPU rack plus a custom UI he calls Chad OS. What‚Äôs interesting isn‚Äôt the hardware‚Äîit‚Äôs how much of his approach works on a single GPU tower or an Apple Silicon laptop. This guide breaks down his setup for people who want to run AI locally without being deep-learning engineers.\nFelix emphasizes two points in the video: ‚ÄúI don‚Äôt want to API my way out of everything,‚Äù and ‚ÄúDelete. Delete. ‚Ä¶ Oh, so they collect your data even if you deleted it.‚Äù Everything below follows that mindset‚Äîlocal-first, privacy-aware experiments you can scale up or down based on your hardware.\nPick the path that matches your hardware, dive deeper with the links, and don‚Äôt feel obligated to chase 8√ó4090 setups."
  },
  {
    "objectID": "posts/everything-you-need-to-know-about-pewdiepie-s-ai-setup/index.html#quick-glossary",
    "href": "posts/everything-you-need-to-know-about-pewdiepie-s-ai-setup/index.html#quick-glossary",
    "title": "Everything you need to know about PewDiePie‚Äôs AI setup",
    "section": "Quick glossary",
    "text": "Quick glossary\n\nQuant (quantization): shrink a model so it fits in laptop memory‚Äîthink of it as zipping weights.\nRAG (retrieval augmented generation): let the model look up your files before it answers you.\nTensor split: share a big model across multiple GPUs so none of them overload."
  },
  {
    "objectID": "posts/everything-you-need-to-know-about-pewdiepie-s-ai-setup/index.html#models-felix-namedropped",
    "href": "posts/everything-you-need-to-know-about-pewdiepie-s-ai-setup/index.html#models-felix-namedropped",
    "title": "Everything you need to know about PewDiePie‚Äôs AI setup",
    "section": "Models Felix name‚Äëdropped",
    "text": "Models Felix name‚Äëdropped\n\nMeta-Llama-3.1-70B-Instruct ‚Äì his first ‚Äúit just works‚Äù checkpoint before moving to larger models.\nopenai/gpt-oss-120b ‚Äì the 120B open model he serves through vLLM for speed.\nQwen/Qwen3-235B-A22B-Instruct-2507 ‚Äì the large model he stretched to 100k tokens of context.\nQwen/Qwen2.5-7B-Instruct and openai/gpt-oss-20b ‚Äì the smaller stand-ins he rotates through daily.\n\nUse the tables below to pick models that fit your machine before you burn time downloading 200B+ checkpoints.\n\nModel quick picks (llama.cpp + MLX)\n\n\n\nModel (quant)\nllama.cpp (bartowski GGUF)\nMLX (mlx-community)\nUnified memory (Mac)\nGPU VRAM (PC/Linux)\nNotes\n\n\n\n\nQwen2.5-7B Instruct Q4_K_M\nbartowski/Qwen2.5-7B-Instruct-GGUF\nmlx-community/Qwen2.5-7B-Instruct-4bit-mlx\n12‚ÄØGB\n8‚ÄØGB\nFast, friendly council member; run this first.\n\n\nGPT-OSS-20B Q4_K_M\nbartowski/GPT-OSS-20B-GGUF\nmlx-community/GPT-OSS-20B-4bit-mlx\n24‚ÄØGB\n12‚ÄØGB\nKeeps 20B quality on laptops with swap.\n\n\nMeta-Llama-3.1-70B Instruct Q4_K_M\nbartowski/Meta-Llama-3.1-70B-Instruct-GGUF\nmlx-community/Meta-Llama-3.1-70B-Instruct-4bit-mlx\n64‚ÄØGB\n48‚ÄØGB\nNeeds tensor splitting or M3 Ultra.\n\n\nGPT-OSS-120B Q4_K_M\nbartowski/GPT-OSS-120B-GGUF\n‚Äî\n96‚ÄØGB\n64‚ÄØGB\nChase this only if you have 4√ó4090 or better cooling.\n\n\nQwen3-235B A22B Q4_K_M\nbartowski/Qwen3-235B-A22B-Instruct-GGUF\n‚Äî\n128‚ÄØGB+\n96‚ÄØGB+\nDemo piece; keep it offline unless you own a rack.\n\n\n\n\n\nDay-to-day downshift\n\nSpin up bartowski/Qwen2.5-7B-Instruct-GGUF or bartowski/GPT-OSS-20B-GGUF for ‚Äúcouncil‚Äù voices while the big models stay idle.\nUse MLX or llama.cpp quantized 13B checkpoints (Mixtral, Llama-3.1-8B) as proxies when 70B/120B VRAM isn‚Äôt available.\nStash a ‚Äúhigh-octane‚Äù profile for vLLM remote runs, but default to 7B‚Äì20B locally so you can iterate quickly.\n\n\nThese memory numbers assume 4-bit quantization with ~25‚ÄØ% headroom for KV cache, runtime buffers, and system processes. Higher precision (Q5/Q8 or FP16) multiplies the requirement.\n\nPrefer MLX-native weights or ready-made .gguf files whenever possible. ‚ÄúConverted weights,‚Äù in Felix‚Äôs terminology, means grabbing a pre-quantized artifact (like the links above) instead of running your own conversion step mid-setup."
  },
  {
    "objectID": "posts/everything-you-need-to-know-about-pewdiepie-s-ai-setup/index.html#llama.cpp-everywhere-cli-webui",
    "href": "posts/everything-you-need-to-know-about-pewdiepie-s-ai-setup/index.html#llama.cpp-everywhere-cli-webui",
    "title": "Everything you need to know about PewDiePie‚Äôs AI setup",
    "section": "1. llama.cpp everywhere (CLI + WebUI)",
    "text": "1. llama.cpp everywhere (CLI + WebUI)\n\nInstall the runtime. Homebrew now ships both CLI and WebUI binaries; Windows/Linux users can download prebuilt archives or make from source:\nbrew install llama.cpp\nLaunch the WebUI with GPT-OSS-20B (fast enough for CPU + GPU hybrids). The WebUI streams weights once and caches them under ~/.cache/llama.cpp. Jump into the Interface pane and enable the collapsible sidebar + preset buttons to match Felix‚Äôs setup:\nllama-server --hf-repo bartowski/GPT-OSS-20B-GGUF \\\n  --hf-file GPT-OSS-20B-Q4_K_M.gguf \\\n  --port 8080 --ctx-size 4096 --threads 10\nOpen http://localhost:8080/?chat for a clean chat UI, provide a system prompt, and pin your favorite sampling presets. --no-browser keeps it silent if you‚Äôre running headless.\nAdd a second route for Qwen2.5-7B so you can flip between ‚Äúcouncil‚Äù members. Each llama-server instance can host multiple models‚Äîadd a second --model block or run another process on a new port. Pair each endpoint with a different --system-prompt or saved preset to recreate his council voting approach:\nllama-server --hf-repo bartowski/Qwen2.5-7B-Instruct-GGUF \\\n  --hf-file qwen2.5-7b-instruct-q4_k_m.gguf \\\n  --port 8081 --ctx-size 8192 --threads 10 \\\n  --chat-template chatml\nPrefer terminals? The CLI flows stay the same‚Äîkeep --batch-size 1 for interactive chats, then scale toward 64+ only when you‚Äôre benchmarking or streaming to multiple clients. Match --ctx-size to the longest prompt you actually need:\nllama-cli --hf-repo bartowski/GPT-OSS-20B-GGUF \\\n  --hf-file GPT-OSS-20B-Q4_K_M.gguf \\\n  --prompt \"Summarize Felix's council experiment in 80 words.\" \\\n  --ctx-size 4096\nllama-cli --hf-repo bartowski/Qwen2.5-7B-Instruct-GGUF \\\n  --hf-file qwen2.5-7b-instruct-q4_k_m.gguf \\\n  --chat --prompt \"Create a persona for a skeptical council member.\" \\\n  --ctx-size 6144\nNeed more throughput? Raise --batch-size gradually (4 ‚Üí 16 ‚Üí 64) while watching latency and GPU memory. Interactive chats usually feel best between batch sizes 1 and 4.\nDual GPUs? Split tensors so each card shares the load‚ÄîFelix‚Äôs ‚Äúcouncil‚Äù runs this way, then a supervisor script scores responses and ‚Äúkills‚Äù the losers. Start with even splits and add a simple vote tally in Python to replicate this:\nllama-cli --hf-repo bartowski/GPT-OSS-20B-GGUF \\\n  --hf-file GPT-OSS-20B-Q4_K_M.gguf \\\n  --tensor-split 50,50 --ctx-size 4096 --chat\n\nThe layout below mirrors Felix‚Äôs WebUI: pinned sidebar, quick preset buttons, and two council members ready to vote.\n\n\n\nReference llama.cpp layout"
  },
  {
    "objectID": "posts/everything-you-need-to-know-about-pewdiepie-s-ai-setup/index.html#apple-silicon-path-mlx",
    "href": "posts/everything-you-need-to-know-about-pewdiepie-s-ai-setup/index.html#apple-silicon-path-mlx",
    "title": "Everything you need to know about PewDiePie‚Äôs AI setup",
    "section": "2. Apple Silicon path: MLX",
    "text": "2. Apple Silicon path: MLX\n\nInstall Apple‚Äôs MLX tooling with uv (fast, no venv juggling):\nuv pip install --upgrade mlx-lm\nPoint MLX straight at the Hugging Face repo IDs‚Äîthe runtime will stream and cache the weights automatically. Swap the --model flag for any entry in the table above:\nmlx_lm.chat --model mlx-community/GPT-OSS-20B-4bit-mlx \\\n  --prompt \"Summarize PewDiePie‚Äôs council idea in 3 bullet points.\"\nmlx_lm.chat --model mlx-community/Qwen2.5-7B-Instruct-4bit-mlx \\\n  --prompt \"List three privacy-first features Felix added to his setup.\"\nFirst run pulls the weights into your Hugging Face cache (usually ~/.cache/huggingface/hub). Swap in larger packs like mlx-community/Meta-Llama-3.1-70B-Instruct-4bit-mlx when you have the VRAM.\n\nFelix emphasizes that ‚Äúsmaller models are amazing‚Äù once you bolt search or RAG on top. On Apple Silicon that means using MLX for the core weights, then piping results through Spotlight, mdfind, or a local embeddings DB before handing the snippets back to the model.\nTip: MLX auto-detects available CPU/GPU tiles. On an M3 Ultra you can bump --max-tokens 2048 safely; older machines should keep generations shorter or drop to 8‚Äë14‚ÄØB models."
  },
  {
    "objectID": "posts/everything-you-need-to-know-about-pewdiepie-s-ai-setup/index.html#no-gpu-use-huggingchat",
    "href": "posts/everything-you-need-to-know-about-pewdiepie-s-ai-setup/index.html#no-gpu-use-huggingchat",
    "title": "Everything you need to know about PewDiePie‚Äôs AI setup",
    "section": "3. No GPU? Use HuggingChat",
    "text": "3. No GPU? Use HuggingChat\nFelix mentions being ‚Äúallergic to cloud APIs,‚Äù but if you‚Äôre still experimenting‚Äîor waiting on your next hardware upgrade‚Äîyou can try the same models through HuggingChat. Pick the gpt-oss or Qwen2.5-72B endpoints, drop in his council/automation prompts, and note the responses, then recreate your favorites locally once you have the compute. HuggingChat keeps a transcript history you can export as JSON, which pairs nicely with the RAG workflows above when you‚Äôre ready to go offline."
  },
  {
    "objectID": "posts/everything-you-need-to-know-about-pewdiepie-s-ai-setup/index.html#multi-gpu-servers-vllm",
    "href": "posts/everything-you-need-to-know-about-pewdiepie-s-ai-setup/index.html#multi-gpu-servers-vllm",
    "title": "Everything you need to know about PewDiePie‚Äôs AI setup",
    "section": "4. Multi-GPU servers: vLLM",
    "text": "4. Multi-GPU servers: vLLM\nIf you have a workstation or cloud box with multiple GPUs, vLLM gives you the same rapid token throughput Felix uses.\npip install -U vllm\n\nvllm serve openai/gpt-oss-120b \\\n  --tensor-parallel-size 4 \\\n  --max-model-len 65536 \\\n  --swap-space 16 \\\n  --dtype auto\nFor Qwen3-235B on eight GPUs:\nvllm serve Qwen/Qwen3-235B-A22B-Instruct-2507 \\\n  --tensor-parallel-size 8 \\\n  --pipeline-parallel-size 2 \\\n  --max-model-len 100000 \\\n  --enforce-eager\nAdd --served-model-name chad-os to keep your OpenAI-compatible clients pointed at custom endpoints. Pair with your own FastAPI or Next.js front-end to recreate his custom UI.\nHe admits that the first time 235B came alive, ‚ÄúI wish I never ran this model. Too much power.‚Äù If you go this big, set conservative batch sizes (--max-num-batches-in-flight 1), add GPU memory monitors, and keep a smaller model on standby for day-to-day prompts so your workstation doesn‚Äôt melt.\n\nCommon snags (and quick fixes)\n\nModel won‚Äôt load? Drop to a smaller quant (Q4 ‚Üí Q3) or close browser tabs to free RAM.\nFans roaring? Cap --max-tokens and set Apple‚Äôs LOW_POWER=1 or NVIDIA PowerMizer to ‚ÄúAdaptive.‚Äù\nWeird answers? Clear the chat history and rerun with a lower temperature like --temp 0.6.\n\nTackle these before you assume your hardware isn‚Äôt strong enough.\n\n‚ÄúI realized I like running AI more than using AI with this computer.‚Äù Build the parts that sound fun and keep the loop private, just like he does.\n\nThat‚Äôs it. Swap in models your hardware can handle, keep the tooling modular, and you‚Äôll have a similar setup running locally without needing 10 GPUs."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I‚Äôm an ML Developer Advocate at Hugging Face focusing on extracting better insights from Text + Audio & on-device ML.\nI‚Äôve been a freelancer, tax analyst, consultant, tech speaker and advisor for five years. In the past three years, I‚Äôve invested significant time volunteering for open source and science organisations like Hugging Face, EuroPython, PyCons across APAC, Google Cloud Delhi, and Facebook Developer Circles.\nDelhi native, I now live in France!\nWhat am I doing now?\nNow"
  },
  {
    "objectID": "about.html#me-in-10-seconds",
    "href": "about.html#me-in-10-seconds",
    "title": "About",
    "section": "",
    "text": "I‚Äôm an ML Developer Advocate at Hugging Face focusing on extracting better insights from Text + Audio & on-device ML.\nI‚Äôve been a freelancer, tax analyst, consultant, tech speaker and advisor for five years. In the past three years, I‚Äôve invested significant time volunteering for open source and science organisations like Hugging Face, EuroPython, PyCons across APAC, Google Cloud Delhi, and Facebook Developer Circles.\nDelhi native, I now live in France!\nWhat am I doing now?\nNow"
  },
  {
    "objectID": "about.html#me-in-10-minutes",
    "href": "about.html#me-in-10-minutes",
    "title": "About",
    "section": "Me in 10 Minutes",
    "text": "Me in 10 Minutes\nI‚Äôm an engineer at heart who likes to fix more things than he breaks. I‚Äôm researching and democratising niche Audio Understanding, Enhancement and Generation systems at the University of Stuttgart & Hugging Face. I‚Äôve consulted across the board, from Startups to Fortune Technology 10 firms, to help them build ML into their day-to-day workflows and make better sense of unstructured data silos. I‚Äôve spent the past five years exploring the world of Data, Linguistics and Math.\nIn addition to my research, I‚Äôve been cultivating Machine Learning for Audio study group with the Hugging Face community. I was also recently made a Hugging Face Fellow for my advocacy work.\nI‚Äôm always game for research collaborations and freelance consulting work. Head to Talks and Consulting to learn how I can create value for you and your organisation. Drop me a line on Twitter (@reach_vb), and we‚Äôll figure out the rest! ;)\n\nTimeline for context\n2014: Marketing + Sales funnel analytics for Microsoft Lumia campaigns in India.\n2016: Freelance Data Analytics + Visualisation for Print + Social Media agencies.\n\nMedium-scale businesses with a focus on creating more re-usable IP\nScaled to mid-5-figure revenue\nShut down in mid-2017 owing to health complications and burnout\n\n2017: Data Science consultant @ Deloitte US-India\n\n12 countries, 3M$+ impact\n\n2020: Moved to Germany - to research NLP & Speech\n2023: Moved to France - to democratise NLP & Speech"
  }
]