<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="VB">
<meta name="dcterms.date" content="2025-11-05">
<meta name="keywords" content="Artificial Intelligence, Machine Learning, Data Science, Personal Blog, Research Notes, llm, llama.cpp, mlx, vllm, gpt-oss, qwen, local-ai, privacy, mac, metal">
<meta name="description" content="TL;DR - Watch Felix Kjellberg’s “STOP. Using AI Right now.” (31 October 2025) to see his 10‑GPU setup and custom UI: https://www.youtube.com/watch?v=qw4fDU18RcU. - You can recreate the workflow with consumer gear—start with 7B–20B quantized models in llama.cpp or Apple’s MLX, then scale up if you add VRAM. - The real goal is autonomy: keep data off third-party APIs, wire in your own search/memory, and iterate until the tooling feels personal.">

<title>Everything you need to know about PewDiePie’s AI setup – VB's Notes and Ramblings</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../images/favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<meta name="robots" content="index, follow">
<meta name="googlebot" content="index, follow">
<meta name="bingbot" content="index, follow">
<meta name="yandex" content="index, follow">
<link rel="alternate" type="application/rss+xml" title="VB’s Notes and Ramblings" href="https://vaibhavs10.github.io/index.xml">
<meta name="description" content="TL;DR - Watch Felix Kjellberg’s “STOP. Using AI Right now.” (31 October 2025) to see his 10‑GPU setup and custom UI: https://www.youtube.com/watch?v=qw4fDU18RcU. - You can recreate the workflow with consumer gear—start with 7B–20B quantized models in llama.cpp or Apple’s MLX, then scale up if you add VRAM. - The real goal is autonomy: keep data off third-party APIs, wire in your own search/memory, and iterate until the tooling feels personal.">
<meta name="author" content="VB">
<meta name="keywords" content="Artificial Intelligence, Machine Learning, Data Science, Personal Blog, Research Notes, llm, llama.cpp, mlx, vllm, gpt-oss, qwen, local-ai, privacy, mac, metal">
<meta property="og:title" content="Everything you need to know about PewDiePie’s AI setup – VB’s Notes and Ramblings">
<meta property="og:description" content="AI and ML field notes, talks, and learnings from Vaibhav/ VB">
<meta property="og:type" content="article">
<meta property="og:url" content="https://vaibhavs10.github.io/posts/everything-you-need-to-know-about-pewdiepie-s-ai-setup/">
<meta property="og:site_name" content="VB’s Notes and Ramblings">
<meta property="og:image" content="https://vaibhavs10.github.io/images/social-card.jpg">
<meta property="og:image:alt" content="VB’s website social card">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:site" content="@reach_vb">
<meta name="twitter:creator" content="@reach_vb">
<meta name="twitter:title" content="Everything you need to know about PewDiePie’s AI setup – VB’s Notes and Ramblings">
<meta name="twitter:description" content="AI and ML field notes, talks, and learnings from Vaibhav/ VB">
<meta name="twitter:image" content="https://vaibhavs10.github.io/images/social-card.jpg">
<link rel="me" href="https://twitter.com/reach_vb">
<link rel="me" href="https://www.linkedin.com/in/vaibhavs10">
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@graph": [
    {
      "@type": "WebSite",
      "@id": "https://vaibhavs10.github.io/#website",
      "url": "https://vaibhavs10.github.io",
      "name": "VB’s Notes and Ramblings",
      "description": "AI and ML field notes, talks, and learnings from Vaibhav/ VB",
      "publisher": {
        "@id": "https://vaibhavs10.github.io/#person"
      }
    },
    {
      "@type": "Person",
      "@id": "https://vaibhavs10.github.io/#person",
      "name": "Vaibhav (VB) Srivastav",
      "url": "https://vaibhavs10.github.io",
      "description": "AI and ML field notes, talks, and learnings from Vaibhav/ VB",
      "image": "https://vaibhavs10.github.io/images/social-card.jpg",
      "sameAs": ["https://twitter.com/reach_vb", "https://www.linkedin.com/in/vaibhavs10"]
    },
    {
      "@type": "BlogPosting",
      "@id": "https://vaibhavs10.github.io/posts/everything-you-need-to-know-about-pewdiepie-s-ai-setup/",
      "url": "https://vaibhavs10.github.io/posts/everything-you-need-to-know-about-pewdiepie-s-ai-setup/",
      "name": "Everything you need to know about PewDiePie’s AI setup – VB’s Notes and Ramblings",
      "description": "TL;DR - Watch Felix Kjellberg’s “STOP. Using AI Right now.” (31 October 2025) to see his 10‑GPU setup and custom UI: https://www.youtube.com/watch?v=qw4fDU18RcU. - You can recreate the workflow with consumer gear—start with 7B–20B quantized models in llama.cpp or Apple’s MLX, then scale up if you add VRAM. - The real goal is autonomy: keep data off third-party APIs, wire in your own search/memory, and iterate until the tooling feels personal.",
      "inLanguage": "en",
      "author": {
        "@id": "https://vaibhavs10.github.io/#person"
      },
      "image": "https://vaibhavs10.github.io/images/social-card.jpg"
    }
  ]
}
</script>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Everything you need to know about PewDiePie's AI setup – VB's Notes and Ramblings">
<meta property="og:description" content="TL;DR - Watch Felix Kjellberg’s “STOP. Using AI Right now.” (31 October 2025) to see his 10‑GPU setup and custom UI: https://www.youtube.com/watch?v=qw4fDU18RcU.">
<meta property="og:image" content="https://vaibhavs10.github.io/images/social-card.jpg">
<meta property="og:site_name" content="VB's Notes and Ramblings">
<meta name="twitter:title" content="Everything you need to know about PewDiePie's AI setup – VB's Notes and Ramblings">
<meta name="twitter:description" content="TL;DR - Watch Felix Kjellberg’s “STOP. Using AI Right now.” (31 October 2025) to see his 10‑GPU setup and custom UI: https://www.youtube.com/watch?v=qw4fDU18RcU.">
<meta name="twitter:image" content="https://vaibhavs10.github.io/images/social-card.jpg">
<meta name="twitter:creator" content="@reach_vb">
<meta name="twitter:site" content="@reach_vb">
<meta name="twitter:card" content="summary_large_image">
<link rel="canonical" href="https://vaibhavs10.github.io/posts/everything-you-need-to-know-about-pewdiepie-s-ai-setup/">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg border-bottom" data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">VB’s Notes and Ramblings</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../now.html"> 
<span class="menu-text">Now</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/reach_vb"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Everything you need to know about PewDiePie’s AI setup</h1>
                  <div>
        <div class="description">
          TL;DR - Watch Felix Kjellberg’s “STOP. Using AI Right now.” (31 October 2025) to see his 10‑GPU setup and custom UI: https://www.youtube.com/watch?v=qw4fDU18RcU. - You can recreate the workflow with consumer gear—start with 7B–20B quantized models in llama.cpp or Apple’s MLX, then scale up if you add VRAM. - The real goal is autonomy: keep data off third-party APIs, wire in your own search/memory, and iterate until the tooling feels personal.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">llm</div>
                <div class="quarto-category">llama.cpp</div>
                <div class="quarto-category">mlx</div>
                <div class="quarto-category">vllm</div>
                <div class="quarto-category">gpt-oss</div>
                <div class="quarto-category">qwen</div>
                <div class="quarto-category">local-ai</div>
                <div class="quarto-category">privacy</div>
                <div class="quarto-category">mac</div>
                <div class="quarto-category">metal</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>VB </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 5, 2025</p>
      </div>
    </div>
    
      
    </div>
    

  <div>
    <div class="keywords">
      <div class="block-title">Keywords</div>
      <p>Artificial Intelligence, Machine Learning, Data Science, Personal Blog, Research Notes, llm, llama.cpp, mlx, vllm, gpt-oss, qwen, local-ai, privacy, mac, metal</p>
    </div>
  </div>
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#quick-glossary" id="toc-quick-glossary" class="nav-link active" data-scroll-target="#quick-glossary">Quick glossary</a></li>
  <li><a href="#models-felix-namedropped" id="toc-models-felix-namedropped" class="nav-link" data-scroll-target="#models-felix-namedropped">Models Felix name‑dropped</a>
  <ul class="collapse">
  <li><a href="#model-quick-picks-llama.cpp-mlx" id="toc-model-quick-picks-llama.cpp-mlx" class="nav-link" data-scroll-target="#model-quick-picks-llama.cpp-mlx">Model quick picks (llama.cpp + MLX)</a></li>
  <li><a href="#day-to-day-downshift" id="toc-day-to-day-downshift" class="nav-link" data-scroll-target="#day-to-day-downshift">Day-to-day downshift</a></li>
  </ul></li>
  <li><a href="#llama.cpp-everywhere-cli-webui" id="toc-llama.cpp-everywhere-cli-webui" class="nav-link" data-scroll-target="#llama.cpp-everywhere-cli-webui">1. llama.cpp everywhere (CLI + WebUI)</a></li>
  <li><a href="#apple-silicon-path-mlx" id="toc-apple-silicon-path-mlx" class="nav-link" data-scroll-target="#apple-silicon-path-mlx">2. Apple Silicon path: MLX</a></li>
  <li><a href="#no-gpu-use-huggingchat" id="toc-no-gpu-use-huggingchat" class="nav-link" data-scroll-target="#no-gpu-use-huggingchat">3. No GPU? Use HuggingChat</a></li>
  <li><a href="#multi-gpu-servers-vllm" id="toc-multi-gpu-servers-vllm" class="nav-link" data-scroll-target="#multi-gpu-servers-vllm">4. Multi-GPU servers: vLLM</a>
  <ul class="collapse">
  <li><a href="#common-snags-and-quick-fixes" id="toc-common-snags-and-quick-fixes" class="nav-link" data-scroll-target="#common-snags-and-quick-fixes">Common snags (and quick fixes)</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>TL;DR - Watch Felix Kjellberg’s “STOP. Using AI Right now.” (31 October 2025) to see his 10‑GPU setup and custom UI: <a href="https://www.youtube.com/watch?v=qw4fDU18RcU" class="uri">https://www.youtube.com/watch?v=qw4fDU18RcU</a>. - You can recreate the workflow with consumer gear—start with 7B–20B quantized models in llama.cpp or Apple’s MLX, then scale up if you add VRAM. - The real goal is autonomy: keep data off third-party APIs, wire in your own search/memory, and iterate until the tooling feels personal.</p>
<p>Felix “PewDiePie” Kjellberg recently shared a video showing his $20K, 10‑GPU rack plus a custom UI he calls Chad OS. What’s interesting isn’t the hardware—it’s how much of his approach works on a single GPU tower or an Apple Silicon laptop. This guide breaks down his setup for people who want to run AI locally without being deep-learning engineers.</p>
<p>Felix emphasizes two points in the video: “I don’t want to API my way out of everything,” and “Delete. Delete. … Oh, so they collect your data even if you deleted it.” Everything below follows that mindset—local-first, privacy-aware experiments you can scale up or down based on your hardware.</p>
<p>Pick the path that matches your hardware, dive deeper with the links, and don’t feel obligated to chase 8×4090 setups.</p>
<section id="quick-glossary" class="level2">
<h2 class="anchored" data-anchor-id="quick-glossary">Quick glossary</h2>
<ul>
<li>Quant (quantization): shrink a model so it fits in laptop memory—think of it as zipping weights.</li>
<li>RAG (retrieval augmented generation): let the model look up your files before it answers you.</li>
<li>Tensor split: share a big model across multiple GPUs so none of them overload.</li>
</ul>
</section>
<section id="models-felix-namedropped" class="level2">
<h2 class="anchored" data-anchor-id="models-felix-namedropped">Models Felix name‑dropped</h2>
<ul>
<li><a href="https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct">Meta-Llama-3.1-70B-Instruct</a> – his first “it just works” checkpoint before moving to larger models.</li>
<li><a href="https://huggingface.co/openai/gpt-oss-120b">openai/gpt-oss-120b</a> – the 120B open model he serves through vLLM for speed.</li>
<li><a href="https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507">Qwen/Qwen3-235B-A22B-Instruct-2507</a> – the large model he stretched to 100k tokens of context.</li>
<li><a href="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct">Qwen/Qwen2.5-7B-Instruct</a> and <a href="https://huggingface.co/openai/gpt-oss-20b">openai/gpt-oss-20b</a> – the smaller stand-ins he rotates through daily.</li>
</ul>
<p>Use the tables below to pick models that fit your machine before you burn time downloading 200B+ checkpoints.</p>
<section id="model-quick-picks-llama.cpp-mlx" class="level3">
<h3 class="anchored" data-anchor-id="model-quick-picks-llama.cpp-mlx">Model quick picks (llama.cpp + MLX)</h3>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Model (quant)</th>
<th>llama.cpp (bartowski GGUF)</th>
<th>MLX (mlx-community)</th>
<th>Unified memory (Mac)</th>
<th>GPU VRAM (PC/Linux)</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Qwen2.5-7B Instruct Q4_K_M</td>
<td><a href="https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF">bartowski/Qwen2.5-7B-Instruct-GGUF</a></td>
<td><a href="https://huggingface.co/mlx-community/Qwen2.5-7B-Instruct-4bit-mlx">mlx-community/Qwen2.5-7B-Instruct-4bit-mlx</a></td>
<td>12 GB</td>
<td>8 GB</td>
<td>Fast, friendly council member; run this first.</td>
</tr>
<tr class="even">
<td>GPT-OSS-20B Q4_K_M</td>
<td><a href="https://huggingface.co/bartowski/GPT-OSS-20B-GGUF">bartowski/GPT-OSS-20B-GGUF</a></td>
<td><a href="https://huggingface.co/mlx-community/GPT-OSS-20B-4bit-mlx">mlx-community/GPT-OSS-20B-4bit-mlx</a></td>
<td>24 GB</td>
<td>12 GB</td>
<td>Keeps 20B quality on laptops with swap.</td>
</tr>
<tr class="odd">
<td>Meta-Llama-3.1-70B Instruct Q4_K_M</td>
<td><a href="https://huggingface.co/bartowski/Meta-Llama-3.1-70B-Instruct-GGUF">bartowski/Meta-Llama-3.1-70B-Instruct-GGUF</a></td>
<td><a href="https://huggingface.co/mlx-community/Meta-Llama-3.1-70B-Instruct-4bit-mlx">mlx-community/Meta-Llama-3.1-70B-Instruct-4bit-mlx</a></td>
<td>64 GB</td>
<td>48 GB</td>
<td>Needs tensor splitting or M3 Ultra.</td>
</tr>
<tr class="even">
<td>GPT-OSS-120B Q4_K_M</td>
<td><a href="https://huggingface.co/bartowski/GPT-OSS-120B-GGUF">bartowski/GPT-OSS-120B-GGUF</a></td>
<td>—</td>
<td>96 GB</td>
<td>64 GB</td>
<td>Chase this only if you have 4×4090 or better cooling.</td>
</tr>
<tr class="odd">
<td>Qwen3-235B A22B Q4_K_M</td>
<td><a href="https://huggingface.co/bartowski/Qwen3-235B-A22B-Instruct-GGUF">bartowski/Qwen3-235B-A22B-Instruct-GGUF</a></td>
<td>—</td>
<td>128 GB+</td>
<td>96 GB+</td>
<td>Demo piece; keep it offline unless you own a rack.</td>
</tr>
</tbody>
</table>
</section>
<section id="day-to-day-downshift" class="level3">
<h3 class="anchored" data-anchor-id="day-to-day-downshift">Day-to-day downshift</h3>
<ul>
<li>Spin up <a href="https://huggingface.co/bartowski/Qwen2.5-7B-Instruct-GGUF">bartowski/Qwen2.5-7B-Instruct-GGUF</a> or <a href="https://huggingface.co/bartowski/GPT-OSS-20B-GGUF">bartowski/GPT-OSS-20B-GGUF</a> for “council” voices while the big models stay idle.</li>
<li>Use MLX or llama.cpp quantized 13B checkpoints (Mixtral, Llama-3.1-8B) as proxies when 70B/120B VRAM isn’t available.</li>
<li>Stash a “high-octane” profile for vLLM remote runs, but default to 7B–20B locally so you can iterate quickly.</li>
</ul>
<blockquote class="blockquote">
<p>These memory numbers assume 4-bit quantization with ~25 % headroom for KV cache, runtime buffers, and system processes. Higher precision (Q5/Q8 or FP16) multiplies the requirement.</p>
</blockquote>
<p>Prefer MLX-native weights or ready-made <code>.gguf</code> files whenever possible. “Converted weights,” in Felix’s terminology, means grabbing a pre-quantized artifact (like the links above) instead of running your own conversion step mid-setup.</p>
</section>
</section>
<section id="llama.cpp-everywhere-cli-webui" class="level2">
<h2 class="anchored" data-anchor-id="llama.cpp-everywhere-cli-webui">1. llama.cpp everywhere (CLI + WebUI)</h2>
<ol type="1">
<li><p>Install the runtime. Homebrew now ships both CLI and WebUI binaries; Windows/Linux users can download prebuilt archives or <code>make</code> from source:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ex">brew</span> install llama.cpp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Launch the WebUI with GPT-OSS-20B (fast enough for CPU + GPU hybrids). The WebUI streams weights once and caches them under <code>~/.cache/llama.cpp</code>. Jump into the <code>Interface</code> pane and enable the collapsible sidebar + preset buttons to match Felix’s setup:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">llama-server</span> <span class="at">--hf-repo</span> bartowski/GPT-OSS-20B-GGUF <span class="dt">\</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">--hf-file</span> GPT-OSS-20B-Q4_K_M.gguf <span class="dt">\</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">--port</span> 8080 <span class="at">--ctx-size</span> 4096 <span class="at">--threads</span> 10</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Open <code>http://localhost:8080/?chat</code> for a clean chat UI, provide a system prompt, and pin your favorite sampling presets. <code>--no-browser</code> keeps it silent if you’re running headless.</p></li>
<li><p>Add a second route for Qwen2.5-7B so you can flip between “council” members. Each <code>llama-server</code> instance can host multiple models—add a second <code>--model</code> block or run another process on a new port. Pair each endpoint with a different <code>--system-prompt</code> or saved preset to recreate his council voting approach:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="ex">llama-server</span> <span class="at">--hf-repo</span> bartowski/Qwen2.5-7B-Instruct-GGUF <span class="dt">\</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">--hf-file</span> qwen2.5-7b-instruct-q4_k_m.gguf <span class="dt">\</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">--port</span> 8081 <span class="at">--ctx-size</span> 8192 <span class="at">--threads</span> 10 <span class="dt">\</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">--chat-template</span> chatml</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Prefer terminals? The CLI flows stay the same—keep <code>--batch-size 1</code> for interactive chats, then scale toward 64+ only when you’re benchmarking or streaming to multiple clients. Match <code>--ctx-size</code> to the longest prompt you actually need:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="ex">llama-cli</span> <span class="at">--hf-repo</span> bartowski/GPT-OSS-20B-GGUF <span class="dt">\</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">--hf-file</span> GPT-OSS-20B-Q4_K_M.gguf <span class="dt">\</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">--prompt</span> <span class="st">"Summarize Felix's council experiment in 80 words."</span> <span class="dt">\</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">--ctx-size</span> 4096</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="sourceCode" id="cb5"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">llama-cli</span> <span class="at">--hf-repo</span> bartowski/Qwen2.5-7B-Instruct-GGUF <span class="dt">\</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">--hf-file</span> qwen2.5-7b-instruct-q4_k_m.gguf <span class="dt">\</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">--chat</span> <span class="at">--prompt</span> <span class="st">"Create a persona for a skeptical council member."</span> <span class="dt">\</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">--ctx-size</span> 6144</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Need more throughput? Raise <code>--batch-size</code> gradually (4 → 16 → 64) while watching latency and GPU memory. Interactive chats usually feel best between batch sizes 1 and 4.</p></li>
<li><p>Dual GPUs? Split tensors so each card shares the load—Felix’s “council” runs this way, then a supervisor script scores responses and “kills” the losers. Start with even splits and add a simple vote tally in Python to replicate this:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">llama-cli</span> <span class="at">--hf-repo</span> bartowski/GPT-OSS-20B-GGUF <span class="dt">\</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">--hf-file</span> GPT-OSS-20B-Q4_K_M.gguf <span class="dt">\</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">--tensor-split</span> 50,50 <span class="at">--ctx-size</span> 4096 <span class="at">--chat</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
</ol>
<p>The layout below mirrors Felix’s WebUI: pinned sidebar, quick preset buttons, and two council members ready to vote.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/chad-os-webui-reference.png" class="img-fluid figure-img" alt="Example llama.cpp WebUI layout with GPT-OSS-20B and Qwen2.5-7B loaded side-by-side, similar to Felix's setup."></p>
<figcaption>Reference llama.cpp layout</figcaption>
</figure>
</div>
</section>
<section id="apple-silicon-path-mlx" class="level2">
<h2 class="anchored" data-anchor-id="apple-silicon-path-mlx">2. Apple Silicon path: MLX</h2>
<ol type="1">
<li><p>Install Apple’s MLX tooling with <code>uv</code> (fast, no venv juggling):</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="ex">uv</span> pip install <span class="at">--upgrade</span> mlx-lm</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div></li>
<li><p>Point MLX straight at the Hugging Face repo IDs—the runtime will stream and cache the weights automatically. Swap the <code>--model</code> flag for any entry in the table above:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="ex">mlx_lm.chat</span> <span class="at">--model</span> mlx-community/GPT-OSS-20B-4bit-mlx <span class="dt">\</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">--prompt</span> <span class="st">"Summarize PewDiePie’s council idea in 3 bullet points."</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="ex">mlx_lm.chat</span> <span class="at">--model</span> mlx-community/Qwen2.5-7B-Instruct-4bit-mlx <span class="dt">\</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">--prompt</span> <span class="st">"List three privacy-first features Felix added to his setup."</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>First run pulls the weights into your Hugging Face cache (usually <code>~/.cache/huggingface/hub</code>). Swap in larger packs like <code>mlx-community/Meta-Llama-3.1-70B-Instruct-4bit-mlx</code> when you have the VRAM.</p></li>
</ol>
<p>Felix emphasizes that “smaller models are amazing” once you bolt search or RAG on top. On Apple Silicon that means using MLX for the core weights, then piping results through Spotlight, <code>mdfind</code>, or a local embeddings DB before handing the snippets back to the model.</p>
<p>Tip: MLX auto-detects available CPU/GPU tiles. On an M3 Ultra you can bump <code>--max-tokens 2048</code> safely; older machines should keep generations shorter or drop to 8‑14 B models.</p>
</section>
<section id="no-gpu-use-huggingchat" class="level2">
<h2 class="anchored" data-anchor-id="no-gpu-use-huggingchat">3. No GPU? Use HuggingChat</h2>
<p>Felix mentions being “allergic to cloud APIs,” but if you’re still experimenting—or waiting on your next hardware upgrade—you can try the same models through <a href="https://huggingface.co/chat">HuggingChat</a>. Pick the <code>gpt-oss</code> or <code>Qwen2.5-72B</code> endpoints, drop in his council/automation prompts, and note the responses, then recreate your favorites locally once you have the compute. HuggingChat keeps a transcript history you can export as JSON, which pairs nicely with the RAG workflows above when you’re ready to go offline.</p>
</section>
<section id="multi-gpu-servers-vllm" class="level2">
<h2 class="anchored" data-anchor-id="multi-gpu-servers-vllm">4. Multi-GPU servers: vLLM</h2>
<p>If you have a workstation or cloud box with multiple GPUs, vLLM gives you the same rapid token throughput Felix uses.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pip</span> install <span class="at">-U</span> vllm</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="ex">vllm</span> serve openai/gpt-oss-120b <span class="dt">\</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">--tensor-parallel-size</span> 4 <span class="dt">\</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">--max-model-len</span> 65536 <span class="dt">\</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>  <span class="at">--swap-space</span> 16 <span class="dt">\</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">--dtype</span> auto</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>For Qwen3-235B on eight GPUs:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="ex">vllm</span> serve Qwen/Qwen3-235B-A22B-Instruct-2507 <span class="dt">\</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">--tensor-parallel-size</span> 8 <span class="dt">\</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">--pipeline-parallel-size</span> 2 <span class="dt">\</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">--max-model-len</span> 100000 <span class="dt">\</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">--enforce-eager</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Add <code>--served-model-name chad-os</code> to keep your OpenAI-compatible clients pointed at custom endpoints. Pair with your own FastAPI or Next.js front-end to recreate his custom UI.</p>
<p>He admits that the first time 235B came alive, “I wish I never ran this model. Too much power.” If you go this big, set conservative batch sizes (<code>--max-num-batches-in-flight 1</code>), add GPU memory monitors, and keep a smaller model on standby for day-to-day prompts so your workstation doesn’t melt.</p>
<section id="common-snags-and-quick-fixes" class="level3">
<h3 class="anchored" data-anchor-id="common-snags-and-quick-fixes">Common snags (and quick fixes)</h3>
<ul>
<li>Model won’t load? Drop to a smaller quant (Q4 → Q3) or close browser tabs to free RAM.</li>
<li>Fans roaring? Cap <code>--max-tokens</code> and set Apple’s <code>LOW_POWER=1</code> or NVIDIA PowerMizer to “Adaptive.”</li>
<li>Weird answers? Clear the chat history and rerun with a lower temperature like <code>--temp 0.6</code>.</li>
</ul>
<p>Tackle these before you assume your hardware isn’t strong enough.</p>
<blockquote class="blockquote">
<p>“I realized I like running AI more than using AI with this computer.” Build the parts that sound fun and keep the loop private, just like he does.</p>
</blockquote>
<p>That’s it. Swap in models your hardware can handle, keep the tooling modular, and you’ll have a similar setup running locally without needing 10 GPUs.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/vaibhavs10\.github\.io");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2025 Vaibhav (VB) Srivastav</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>