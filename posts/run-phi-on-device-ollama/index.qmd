---
title: "Run Phi-4 with ollama and Hugging Face"
author: "VB"
date: "2025-01-16"
description: "Get Microsoft's most accurate Phi-4 running locally on your Mac in minutes. No GPU required."
categories: [llm, ollama, mac, metal, cuda, phi4, phi]
toc: true
---

This is going to be a short post or rather a testlog for how to run the most accurate version of Microsoft Phi-4 on your Mac.

::: {.callout-card data-label="Why Phi-4" data-tone="accent"}
Phi-4 is surprisingly strong at reasoning and code for its size. With Ollama and Hugging Face you can treat this post as a lab notebook—copy the commands, tweak a flag, rerun, observe.
:::

## Step 1: Setup ollama

On a mac, you can install ollama using homebrew:

```bash
brew install ollama
```

On a Windows/ Linux device, you can follow the instructions on the [Ollama Docs](https://ollama.com/download))

## Step 2: Kickstart ollama

```bash
ollama serve
```

## Step 3: Run inference with Phi-4

After some research I found that the Phi-4 GGUFs from Unsloth are the most accurate. They ran bunch of evals and also converted the model to LLaMa format.
You can find it here: [unsloth/phi-4-GGUF](https://huggingface.co/unsloth/phi-4-GGUF).

```bash
ollama run hf.co/unsloth/phi-4-GGUF
```

I'd also recommend reading the blogpost about the fixes for the Phi-4 model [here](https://unsloth.ai/blog/phi4).

::: {.side-note}
Tip: use `ollama run hf.co/unsloth/phi-4-GGUF:Q4` if you’d like a lighter quantisation on smaller laptops.
:::

## Step 4: Use it for your own tasks

That's the fun bit, once the model is loaded, you can do whatever you want, at the touch of your terminal.

Go on and try out some of your own prompts, and see how it works.

![Chatting with Phi 4 on my Macbook with Ollama](example.png)

Bonus: Now go try other [GGUF models on the Hub](https://huggingface.co/models?library=gguf) and compare their performance with Phi 4.

and.. that's it!

Oh, sorry, one last thing, you can now even run private GGUFs from the Hugging Face Hub via ollama, [read here](https://huggingface.co/docs/hub/en/ollama).

::: {.callout-card data-label="What next" data-tone="warm"}
- Wire the Ollama HTTP server into your favorite TUI client and script prompts from a notes app.
- Capture prompts/responses into a local SQLite db so you can diff quantisation quality across experiments.
:::